Sys.getlocale()
assignment1 <- c(15,77,56,12,45,36,7,99,82,63)
mean(assignment1)
median(assignment1)
sd(assignment1)
install.packages(tidyverse)
install.packages("tidyverse")
install.packages("dplyr")
2^1.5
2^2
install.packages("pdftools")
install.packages(tm)
install.packages("tm")
install.packages("Rpoppler")
Sys.getenv()
Sys.getlocale()
Sys.setlocale("LC_ALL","English")
Sys.setlocale("LC_ALL","English")
Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
Sys.getlocale()
?Sys.setlocale
Sys.getlocale(category = "LC_ALL")
Sys.setlocale(category = "LC_ALL", locale = "")
Sys.setlocale(category = "LC_ALL", locale = "en")
install.koRpus.lang("en")
# load the package
library(koRpus.lang.en)
install.koRpus.lang("en")
library(koRpus.lang.en)
Sys.setlocale(category = "LC_ALL", locale = "en")
install.packages("tm")
install.packages("qdap")
install.packages("wordcloud")
install.packages("wordcloud2")
install.packages("RColorBrewer")
######## set up all the packages
library(tokenizers)
library(tm)
library(rJava)
library(openNLP)
library(coreNLP)
require("NLP")
## must run this one first
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
####### load data
options(java.parameters = "- Xmx1024m")
load("workspaces/CSR_documents_30samples.RData")
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
## must run this one first
#options(java.parameters = "- Xmx1024m")
library(XLConnect)
## must run this one first
#options(java.parameters = "- Xmx1024m")
install.packages("XLConnect")
library(XLConnect)
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
## must run this one first
#options(java.parameters = "- Xmx1024m")
#install.packages("XLConnect")
#library(XLConnect)
initCoreNLP(type = "english", parameterFile = NULL,
mem = "8g")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
## generate data
x1<- rnrom(200)
library(tidyverse)
## generate data
x1<- rnrom(200)
## generate data
x1<- rnorm(200)
x2<- rnorm(200)
??rnorm
y<-  -2+1.5*x1-0.8*x2+rnorm(200)
dat<- data.frame(y,x1,x2)
## random split to training and testing
index<- sample(nrow(dat),0.8*nrow(dat))
train<- dat[index,]
test<- dat[-index,]
#GoHawks#2020
install.packages("odbc")
install.packages("Sleuth2")
library(Sleuth2)
head(ex1029)
model = lm(wage~Education+Experience+Black+SMSA+Region, data = ex1029)
model = lm(Wage~Education+Experience+Black+SMSA+Region, data = ex1029)
summary(model)
install.packages("alr3")
library(alr3)
fuel2001 <- fuel2001
View(fuel2001)
m1 = lm(fuel2001$FuelC~., data = fuel2001)
summary(m1)
par(mfrow = c(2, 2))
plot(m1)
remove_spots <- which(row.names(fuel2001) %in% c("FL","TX","NY"))
no_tx_fl_ny <- fuel2001[-remove_spots,]
new_model <- lm(FuelC ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(new_model)
summary(m1)
attach(fuel2001)
m1 = lm(fuel2001$FuelC~Tax + Drivers + Income + log(Miles,10), data = fuel2001)
summary(m1)
par(mfrow = c(2, 2))
plot(m1)
hist(fuel2001$FuelC)
m2 <- lm(log(FuelC) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
m2 <- lm(log(FuelC,10) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
m2 <- lm(log(FuelC) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
exp(coefficients(m2))
plot(m2)
par(mfrow = c(2, 2))
plot(m2)
load("D:/1Github/CSRtm/workspaces/Specificity_Scores.RData")
house_data <- read.csv("http://www.lock5stat.com/datasets/HomesForSale.csv")
log_price <- log(house_data$Price, 10)
m1<- lm(log_price~Size+Beds, data = house_data)
summary(m1)
m2<- lm(log_price~Size+Beds+Baths, data = house_data)
summary(m2)
m3<- lm(log_price~Size+Beds+Baths+State, data = house_data)
summary(m1)
summary(m3)
anova(m1,m2)
anova(m2,m3)
AIC(m1,m2,m3)
help(state.x77)
state_data <- data.frame(state.x77)
state_data
pairs(state_data)
no_alaska <- state_data[-2,]
inter_model <- lm(Life.Exp ~1,no_alaska)
forward_model <- step(inter_model,direction = "forward",scope = (~Population+Income+Illiteracy+Murder+HS.Grad+Frost+Area))
final<-lm(Life.Exp ~ Murder + HS.Grad + Frost + Population, data = no_alaska)
summary(final)
runif(1)
?pnorm
pnorm(15,14,2)-pnorm(10,14,2)
1-pnorm(18,14,2)
qnorm(0.84,14,2)
### when it refers to a posibility, it says the midle area, gotta add half of the missing sample back
### if we do not know the population SD, we have to get tstar
### sample size - 1 would be the second value
### this example is using 80% confidence and 16(n) sample size
qt(0.9,15)
?qt
qt(0.005,3000)
qt(0.005,15)
pnorm(14,15,3.863288754)
1-pnorm(14,15,3.863288754)
1-qnorm(14,15,3.863288754)
1-qnorm(0.005,15,3.863288754)
qnorm(0.005,15,3.863288754)
### when it refers to a posibility, it says the midle area, gotta add half of the missing sample back
### if we do not know the population SD, we have to get tstar
### sample size - 1 would be the second value
### this example is using 80% confidence and 16(n) sample size
qt(0.9,15)
?qt
qt(0.995,2999)
qt(-2,9)
### when it refers to a posibility, it says the midle area, gotta add half of the missing sample back
### if we do not know the population SD, we have to get tstar
### sample size - 1 would be the second value
### this example is using 80% confidence and 16(n) sample size
qt(1,15)
dt(1,9)
dt(1,9)-dt(-2,9)
pt(1,9)-dt(-2,9)
pt(1,9)
dt(-2,9)
rt(1,9)-rt(-2,9)
pt(1,9)
dt(1,9)
1-dt(1,9)
qt(1,9)
pt(1,9)
pt(1,10)
pt(1,10)-pt(-2,10)
pt(1,100)-pt(-2,100)
qt(-2,rt)
qt(-2,rt())
qt(0.68,20)
qt(0.68,3)
tstar <- qt(.975,34)
p_lb <- 66.56 - tstar*4.62*(1+1/sqrt(35))
p_ub <- 66.56 + tstar*4.62*(1+1/sqrt(35))
pred <- c(p_lb, p_ub)
pred
tstar<- qt(0.975,34)
uk_lb <- 66.56-tstar*4.62/sqrt(35)
uk_ub <- 66.56+tstar*4.62/sqrt(35)
a_uk <- c(uk_lb, uk_ub)
a_uk
tstar<- qt(0.975,34)
uk_lb <- 67.77-tstar*6.24/sqrt(35)
uk_ub <- 67.77+tstar*6.24/sqrt(35)
a_uk <- c(uk_lb, uk_ub)
a_uk
0.36-1.96*sqrt(0.36*0.64/900)
0.36+1.96*sqrt(0.36*0.64/900)
qt(0.975,60)
-0.02-0.39548*2.000298
-0.02+0.39548*2.000298
-0.02-1.96*sqrt((0.14^2/31)+(0.17^2/31))
-0.02+1.96*sqrt((0.14^2/31)+(0.17^2/31))
tstar <- qt(.975,59)
p_lb <- 95720 - tstar*22621*(1+1/sqrt(60))
p_ub <- 95720 + tstar*22621*(1+1/sqrt(60))
pred <- c(p_lb, p_ub)
pred
tstar<- qt(0.975,59)
uk_lb <- 95720-tstar*22621/sqrt(60)
uk_ub <- 95720+tstar*22621/sqrt(60)
a_uk <- c(uk_lb, uk_ub)
a_uk
tstar<- qt(0.975,66)
uk_lb <- 104474.6-tstar*20871.22/sqrt(67)
uk_ub <- 104474.6+tstar*20871.22/sqrt(67)
a_uk <- c(uk_lb, uk_ub)
a_uk
ConfidenceInterval = function(confidence,samplesize,average,stdDev){
tstar<- qt(confidence,samplesize-1)
uk_lb <- average-tstar*stdDev/sqrt(samplesize)
uk_ub <- average+tstar*stdDev/sqrt(samplesize)
a_uk <- c(uk_lb, uk_ub)
a_uk
}
ConfidenceInterval(0.975,67,104474.6,20871.22)
## Condidence Interval
ConfidenceInterval = function(confidence,samplesize,average,stdDev){
tstar<- qt(confidence,samplesize-1)
uk_lb <- average-tstar*stdDev/sqrt(samplesize)
uk_ub <- average+tstar*stdDev/sqrt(samplesize)
a_uk <- c(uk_lb, uk_ub)
a_uk
}
ConfidenceInterval(0.975,250,0.536,)
## Test statistic
Z<- (19.6-23)/(7.3/sqrt(227))
## approach 1
p_value<- pnorm(Z)
p_value
x<-1472.5
s<-833.477
t<-(x-1350)/(s/sqrt(100))
t
2*pt(-t,99)
library(MASS)
2+2
setwd("D:/OneDrive/ASU/2021 Spring/Applied Project/ASU_Applied_Project_2021/Data")
knitr::opts_chunk$set(echo = TRUE)
library(fpc)
set.seed(2021)
emb_df = read.csv("embedding1.csv")
setwd("D:/OneDrive/ASU/2021 Spring/Applied Project/ASU_Applied_Project_2021/Data")
library(fpc)
set.seed(2021)
emb_df = read.csv("embedding1.csv")
knitr::opts_chunk$set(echo = TRUE)
library(fpc)
set.seed(2021)
emb_df = read.csv("embedding1.csv")
head(emb_df)
fit <- kmeans(emb_df[,-1], 5,)
# Cluster
fit$cluster
# Summary
summary(fit)
# Names
names(fit)
subsample <- list()
for(i in 1:5){
subsample[[i]]<- emb_df[fit$cluster==i,-1]
}
fit$centers[1:20]
apply(subsample[[1]], 2, mean)
########## choice of K
wss<- NULL
for (i in 1:10){
fit1=kmeans(emb_df[,-1],centers = i)
wss=c(wss, fit1$tot.withinss)
}
plot(1:10, wss, type = "o")
fit2 <- kmeans(emb_df[,-1], 9,)
plotcluster(emb_df[,-1],fit$cluster)
plotcluster(emb_df[,-1],fit2$cluster)
fit2 <- kmeans(emb_df[,-1], 4,)
plotcluster(emb_df[,-1],fit$cluster)
plotcluster(emb_df[,-1],fit2$cluster)
## calculate the distance matrix
emb.dist<- dist(emb_df[,-1])
#obtain clusters
emb.hcluster<- hclust(emb.dist)
plot(emb.hcluster)
fit2 <- kmeans(emb_df[,-1], 2,)
plotcluster(emb_df[,-1],fit$cluster)
plotcluster(emb_df[,-1],fit2$cluster)
fit2 <- kmeans(emb_df[,-1], 3,)
plotcluster(emb_df[,-1],fit$cluster)
plotcluster(emb_df[,-1],fit2$cluster)
fit2 <- kmeans(emb_df[,-1], 4,)
plotcluster(emb_df[,-1],fit$cluster)
plotcluster(emb_df[,-1],fit2$cluster)
fit <- kmeans(emb_df[,-1], 5,)
# Cluster
fit$cluster
# Summary
summary(fit)
# Names
names(fit)
#tot.withinss
fit$tot.withinss
sub<-rownames(subsample[[1]])
gsub("\"","",sub, fixed = TRUE )
subset1<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[1]]))
subset2<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[2]]))
subset3<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[3]]))
subset4<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[4]]))
subset
subset1$X
Net<-read.csv("NetworkAnalysis1.csv")
head(Net)
Group1<-as.data.frame(unique(Net[Net$Celebrity %in% subset1$Source, "Celebrity"]))
Group2<-as.data.frame(unique(Net[Net$Celebrity %in% subset2$Source, "Celebrity"]))
Group3<-as.data.frame(unique(Net[Net$Celebrity %in% subset3$Source, "Celebrity"]))
Group4<-as.data.frame(unique(Net[Net$Celebrity %in% subset4$Source, "Celebrity"]))
write.csv(Group1, file = "Output/Group1.csv")
write.csv(Group1, file = "Output/Group1.csv")
write.csv(Group2, file = "Output/Group2.csv")
write.csv(Group3, file = "Output/Group3.csv")
write.csv(Group4, file = "Output/Group4.csv")
View(Group1)
View(Group2)
subsample <- list()
for(i in 1:4){
subsample[[i]]<- emb_df[fit2$cluster==i,-1]
}
View(subsample)
sub<-rownames(subsample[[1]])
gsub("\"","",sub, fixed = TRUE )
subset1<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[1]]))
subset2<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[2]]))
subset3<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[3]]))
subset4<- subset(emb_df, rownames(emb_df) %in% rownames(subsample[[4]]))
View(subset1)
View(subset1)
Net<-read.csv("NetworkAnalysis1.csv")
head(Net)
Group1<-as.data.frame(unique(Net[Net$Celebrity %in% subset1$X, "Celebrity"]))
Group2<-as.data.frame(unique(Net[Net$Celebrity %in% subset2$X, "Celebrity"]))
Group3<-as.data.frame(unique(Net[Net$Celebrity %in% subset3$X, "Celebrity"]))
Group4<-as.data.frame(unique(Net[Net$Celebrity %in% subset4$X, "Celebrity"]))
View(Group1)
View(Group2)
View(Group3)
View(Group4)
write.csv(Group1, file = "Output/Group1.csv")
write.csv(Group2, file = "Output/Group2.csv")
write.csv(Group3, file = "Output/Group3.csv")
write.csv(Group4, file = "Output/Group4.csv")
